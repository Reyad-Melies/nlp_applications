# -*- coding: utf-8 -*-
"""Final of Assignment 3 - group - nlp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_SBLNdmeiDdCgIKkVCvhna4hIcfnObSC
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
import random
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
import wordcloud
from sklearn import metrics
from sklearn.svm import SVC
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, accuracy_score
from sklearn.model_selection import cross_val_score

nltk.download('gutenberg')
nltk.download("stopwords")
nltk.download('punkt')
nltk.download('wordnet')

text=nltk.corpus.gutenberg.raw('chesterton-thursday.txt')
tokenized_sents=nltk.sent_tokenize(text)

#global dataFrameT
dataFrameT = pd.DataFrame()

def listToString(s): 
  text = " ".join(s)
  return text

def stringOrList(x):
  if type(x) == str:
   textDataPartitioning(x)

  elif type(x) == list:
    for i in range(len(x)):
      textDataPartitioning(x[i])

def stemming_text(words):
  stemmer = PorterStemmer()
  # words = text.split()
  words = [stemmer.stem(word.strip()) for word in words]
  return " ".join(words)

def lemm_text(words):
  lemmatizer = WordNetLemmatizer()
  # words = text.split()
  words = [lemmatizer.lemmatize(word, pos='v') for word in words]
  return " ".join(words)

def textDataPartitioning(x): 
 text = nltk.corpus.gutenberg.raw(x)
 stop_words = set(stopwords.words('english')) 

 text = re.sub(r'[^\w\s]', '', str(text).lower().strip())
 text = re.sub(r'[\d_]', '', text)

 print("--- removing stop words ---")           #stop words must be removed first
 word_tokens = word_tokenize(text)
 filtered_words = [] 
 for w in word_tokens: 
     if w not in stop_words: 
         filtered_words.append(w)

 print("--- stemming ---")                       #stemming is supposed to show better performance than lemmatization but we will test both
 new_text = stemming_text(filtered_words)

#  print("--- lemmatization ---") 
#  new_text = lemm_text(filtered_words)

 filtered_sentence = (new_text.strip()).split()

 temp = []
 list_of_lists = []
 list_of_lists1 = []
 index=0

 for word in filtered_sentence:
   temp.append(word)
   if len(temp) == 100:
    list_of_lists.append(temp)
    temp=[]

 for i in range(200):
  ran = random.randint(0, len(list_of_lists)-1)
  list_of_lists1.append([listToString(list_of_lists[ran]), x])

 dataFrame = pd.DataFrame(list_of_lists1, columns=["partition", "book"])
 global dataFrameT  
 dataFrameT = dataFrameT.append(dataFrame, ignore_index = True)

def string_to_integer(s):
  mydict={}
  i = 0
  for item in s.unique():
    if(i>0 and item in mydict):
        continue
    else:    
       i = i+1
       mydict[item] = i
  k=[]
  for item in s:
    k.append(mydict[item])
  return k

#list of 100 words in first column
# book_list = nltk.corpus.gutenberg.fileids()
# random_book_index = random.sample(range(0, len(book_list) - 1), 5)         # get 5 random books

# book_list = ['chesterton-thursday.txt', 'austen-emma.txt']

# romance_genre = ['austen-persuasion.txt', 'austen-sense.txt']              # same authher can't be used
poetry_genre = ['blake-poems.txt', 'milton-paradise.txt', 'whitman-leaves.txt']
fiction_genre = ['burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-thursday.txt', 'melville-moby_dick.txt']
# tragedy_genre = ['shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt']    # same author can't be used


# random_book_list = []

# for i in random_book_index:
#   random_book_list.append(book_list[i])

stringOrList(fiction_genre)
print(dataFrameT)

DataFrameOrg = dataFrameT.copy()

dataFrameT = DataFrameOrg.copy()   #use this line to get the same data you got before

dataFrameT['word_count'] = dataFrameT["partition"].apply(lambda x: len(str(x).split(" ")))
dataFrameT['char_count'] = dataFrameT["partition"].apply(lambda x: sum(len(word) for word in str(x).split(" ")))
dataFrameT['sentence_count'] = dataFrameT["partition"].apply(lambda x: len(str(x).split(".")))
dataFrameT['avg_word_length'] = dataFrameT['char_count'] / dataFrameT['word_count']
dataFrameT['avg_sentence_lenght'] = dataFrameT['word_count'] / dataFrameT['sentence_count']
dataFrameT['Author'] = dataFrameT['book'].apply(lambda x: x.split('-')[0])
dataFrameT

from textblob import TextBlob
dataFrameT["Sentiment"] = dataFrameT["partition"].apply(lambda x: 
                   TextBlob(x).sentiment.polarity)
dataFrameT

dataFrameT.shape

top=10
nltk.download('punkt')
## for vectorizer
from sklearn import feature_extraction, manifold

corpus = dataFrameT["partition"]
lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=" "))
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle("Most frequent words", fontsize=15)
    
## unigrams
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[0], 
                  legend=False).grid(axis='x')
ax[0].set(ylabel=None)
    
## bigrams
dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))
dtf_bi = pd.DataFrame(dic_words_freq.most_common(), 
                      columns=["Word","Freq"])
dtf_bi["Word"] = dtf_bi["Word"].apply(lambda x: " ".join(
                   string for string in x) )
dtf_bi.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Bigrams", ax=ax[1],
                  legend=False).grid(axis='x')
ax[1].set(ylabel=None)
plt.show()

"""Visualization:"""

wc = wordcloud.WordCloud(background_color='black', max_words=100, 
                         max_font_size=35)
wc = wc.generate(str(corpus))
fig = plt.figure(num=1)
plt.axis('off')
plt.imshow(wc, cmap=None)
plt.show()

import seaborn as sp
def plott(label,y_pred):
  clf_report = classification_report(label,
                                   y_pred,
                                   target_names=['burgess','carroll','chesterton','melville'],
                                   output_dict=True)
  print('\nConfusion Matrix:\n' )
  print(multilabel_confusion_matrix(label, y_pred))
  sp.heatmap(confusion_matrix(label, y_pred))
  plt.show()
  print('\nClassification Report:\n' )
  print(classification_report(label, y_pred))
  sp.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)
  plt.show()

import matplotlib.pyplot as plt
def funp(y,yp):
  plt.plot(y,'x')
  plt.plot(yp, 'o')
  plt.show()

"""Tokenization and Transformation BOW

Count Vectorization with ngram
"""

from sklearn.feature_extraction.text import CountVectorizer          # we will use the count vectorizer dataset as input as a first test
count_vect = CountVectorizer(ngram_range = (1, 2), max_features=2500)                   # you can change the number of n-grams while testing
X_train_counts = count_vect.fit_transform(dataFrameT["partition"])
print(X_train_counts.shape)

dataset_countV = pd.DataFrame(X_train_counts.toarray(), columns = count_vect.get_feature_names())
sentiment = pd.DataFrame(data = dataFrameT.loc[:,"Sentiment"], columns = ['Sentiment'])
dataset_countV = dataset_countV.join(sentiment, rsuffix = '_score')
dataset_countV

count_vect.vocabulary_.get(u'algorithm')

"""Count Vectorization without ngram (BoW)"""

count_v = CountVectorizer(max_features=2500)
X_train_counts_v = count_v.fit_transform(dataFrameT["partition"])
print(X_train_counts_v.shape)

dataset_CV_without_ngram = pd.DataFrame(X_train_counts_v.toarray(), columns = count_v.get_feature_names())
dataset_CV_without_ngram = dataset_CV_without_ngram.join(sentiment, rsuffix = '_score')
dataset_CV_without_ngram

"""# TFIDF Vectorization

### TFIDF Vectoriation with n-grams
"""

from sklearn.feature_extraction.text import TfidfVectorizer          # we will use the TFIDF vectorizer with n-grams dataset as input as a second test
tfidf_vectorizer_ngram = TfidfVectorizer(use_idf=True, ngram_range = (1, 2), max_features = 2500) 
X_train_tfidf_ngram = tfidf_vectorizer_ngram.fit_transform(dataFrameT["partition"])
X_train_tfidf_ngram.shape

dataset_tfidfV_ngram = pd.DataFrame(X_train_tfidf_ngram.toarray(), columns = tfidf_vectorizer_ngram.get_feature_names())
dataset_tfidfV_ngram = dataset_tfidfV_ngram.join(sentiment, rsuffix = '_score')
dataset_tfidfV_ngram

"""TFIDF Vectoriation without n-grams"""

tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_features=2500)          # we will use the TFIDF vectorizer without n-grmas dataset as input as a third test
X_train_tfidf = tfidf_vectorizer.fit_transform(dataFrameT["partition"])
X_train_tfidf.shape

dataset_tfidfV = pd.DataFrame(X_train_tfidf.toarray(), columns = tfidf_vectorizer.get_feature_names())
dataset_tfidfV = dataset_tfidfV.join(sentiment, rsuffix = '_score')
dataset_tfidfV

"""# Shuffling data

### count vectorizer with ngram
"""

# first we join the author column
author = dataFrameT["Author"]
dataset_countV = dataset_countV.join(author, rsuffix = "_")

# then we shuffle the data
dataset_countV = dataset_countV.sample(frac = 1)

# then we split the input and the target
dataset_countV_target = dataset_countV['Author']
dataset_countV_input = dataset_countV.drop(columns = ['Author'])

# dataset_countV_target

"""### count vectorizer without ngram"""

# first we join the author column
dataset_CV_without_ngram = dataset_CV_without_ngram.join(author, rsuffix = "_")

# then we shuffle the data
dataset_CV_without_ngram = dataset_CV_without_ngram.sample(frac = 1)

# then we split the input and the target
dataset_CV_without_ngram_target = dataset_CV_without_ngram['Author']
dataset_CV_without_ngram_input = dataset_CV_without_ngram.drop(columns = ['Author'])

# dataset_countV_target

"""### TFIDF with ngrams"""

# first we join the author column
dataset_tfidfV_ngram = dataset_tfidfV_ngram.join(author, rsuffix = "_")

# then we shuffle the data
dataset_tfidfV_ngram = dataset_tfidfV_ngram.sample(frac = 1)

# then we split the input and the target
dataset_tfidfV_ngram_target = dataset_tfidfV_ngram['Author']
dataset_tfidfV_ngram_input = dataset_tfidfV_ngram.drop(columns = ['Author'])

# dataset_tfidfV_ngram_target

"""### TFIDF without ngrams"""

# first we join the author column
dataset_tfidfV = dataset_tfidfV.join(author, rsuffix = "_")

# then we shuffle the data
dataset_tfidfV = dataset_tfidfV.sample(frac = 1)

# then we split the input and the target
dataset_tfidfV_target = dataset_tfidfV['Author']
dataset_tfidfV_input = dataset_tfidfV.drop(columns = ['Author'])

# dataset_tfidfV_input

>>> from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
dataset_countV_input = scaler.fit_transform(dataset_countV_input)
dataset_tfidfV_ngram_input = scaler.fit_transform(dataset_tfidfV_ngram_input)
dataset_tfidfV_input = scaler.fit_transform(dataset_tfidfV_input)
dataset_CV_without_ngram_input = scaler.fit_transform(dataset_CV_without_ngram_input)

"""## Splitting the data"""

from sklearn.model_selection import train_test_split

#count vectorizer with ngram
X_train_countV, X_test_countV, y_train_countV, y_test_countV = train_test_split(dataset_countV_input, dataset_countV_target, test_size=0.4, random_state=0)

#tfidf with ngrams
X_train_tfidfV_ngram, X_test_tfidfV_ngram, y_train_tfidfV_ngram, y_test_tfidfV_ngram = train_test_split(dataset_tfidfV_ngram_input, dataset_tfidfV_ngram_target, test_size=0.4, random_state=0)

#tdidf without ngrams
X_train_tfidfV, X_test_tfidfV, y_train_tfidfV, y_test_tfidfV = train_test_split(dataset_tfidfV_input, dataset_tfidfV_target, test_size=0.4, random_state=0)

#count vectorizer without ngram
X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(dataset_CV_without_ngram_input, dataset_CV_without_ngram_target, test_size=0.4, random_state=0)

"""# Support vector machine Classification

counter Vectorization
"""

# cross_validation
clf = SVC(kernel= 'linear').fit(X_train_countV, y_train_countV)
print('accuracy of cross validation: ', cross_val_score(clf, X_train_countV, y_train_countV, cv=10).mean())
dataSVCCV=clf.predict(X_test_countV)
#y_test_countV
print ('accuracy of test: {}'.format(clf.score(X_test_countV, y_test_countV)))

#y_test_countV

plott(dataSVCCV,y_test_countV)

"""TF-IDF"""

from sklearn.metrics import multilabel_confusion_matrix
from sklearn.svm import SVC
import numpy as np
# cross_validation
clf = SVC(kernel= 'linear').fit(X_train_tfidfV, y_train_tfidfV)
print('accuracy of cross validation: ', cross_val_score(clf, X_train_tfidfV, y_train_tfidfV, cv=10).mean())

# Evaluation
predicted = clf.predict(X_test_tfidfV)
# print(np.mean(predicted == y_test_tfidfV))
print ('accuracy of test: {}'.format(clf.score(X_test_tfidfV, y_test_tfidfV)))

plott(predicted,y_test_tfidfV)

# import seaborn as sns
# x=predicted.transpose()
# sns.boxplot(x)
# plt.show()

"""TF-IDF/N-Grams"""

clf = SVC(kernel= 'linear').fit(X_train_tfidfV_ngram, y_train_tfidfV_ngram)
print('accuracy of cross validation: ', cross_val_score(clf, X_train_tfidfV_ngram, y_train_tfidfV_ngram, cv=10).mean())
predicted1 = clf.predict(X_test_tfidfV_ngram)
print ('accuracy of test: {}'.format(clf.score(X_test_tfidfV_ngram, y_test_tfidfV_ngram)))

pip install mlxtend --upgrade --no-deps

from mlxtend.evaluate import bias_variance_decomp
X_train_tfidfV_ngram, X_test_tfidfV_ngram, y_train_tfidfV_ngram, y_test_tfidfV_ngram = train_test_split(X_train_tfidf_ngram , dataset_tfidfV_ngram_target, test_size=0.4, random_state=0)

# # estimate bias and variance
clf = SVC(kernel= 'linear')
mse, bias, var = bias_variance_decomp(clf, X_train_tfidfV_ngram, np.array(string_to_integer(y_train_tfidfV_ngram)), X_test_tfidfV_ngram, np.array(string_to_integer(y_test_tfidfV_ngram)), loss='mse', num_rounds=200, random_seed=1)
# # summarize results
print('MSE: %.3f' % mse)
print('Bias: %.3f' % bias)
print('Variance: %.3f' % var)

plott(predicted1,y_test_tfidfV_ngram)

"""count vectorizer without N-gram"""

clf = SVC(kernel= 'linear').fit(X_train_cv, y_train_cv)
print('accuracy of cross validation: ', cross_val_score(clf, X_train_cv, y_train_cv, cv=10).mean())
predicted1 = clf.predict(X_test_cv)
print ('accuracy of test: {}'.format(clf.score(X_test_cv, y_test_cv)))

plott(predicted1,y_test_cv)

"""# Decision tree Model

Count vectorizer with ngram
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import ShuffleSplit, cross_val_score, cross_val_predict

tree_model = DecisionTreeClassifier(random_state=0)
tree_model.fit(X_train_countV, y_train_countV)

scores = cross_val_score(tree_model, X_train_countV, y_train_countV, cv= 10 )
print('Accuracy of cross-validation: ' , scores.mean())

y_pred = tree_model.predict(X_test_countV)

print('Accuracy of test', tree_model.score(X_test_countV, y_test_countV))

#print('Accuracy of the model', accuracy_score(y_test_countV, y_pred))

plott(y_test_countV, y_pred)

"""Count vectorizer without ngram"""

tree_model_cv = DecisionTreeClassifier(random_state=0)
tree_model_cv.fit(X_train_cv, y_train_cv)

scores = cross_val_score(tree_model_cv, X_train_cv, y_train_cv, cv= 10 )
print('Accuracy of cross-validation: ' , scores.mean())

y_pred = tree_model_cv.predict(X_test_cv)

print('Accuracy of test', tree_model_cv.score(X_test_cv, y_test_cv))

plott(y_test_cv, y_pred)

"""tfidf with ngrams"""

tree_model_tf_n = DecisionTreeClassifier(random_state=0)
tree_model_tf_n.fit(X_train_tfidfV_ngram, y_train_tfidfV_ngram)

scores = cross_val_score(tree_model_tf_n, X_train_tfidfV_ngram, y_train_tfidfV_ngram, cv= 10)
print('Accuracy of cross validation: ' , scores.mean())

y_pred = tree_model_tf_n.predict(X_test_tfidfV_ngram)

#print('Accuracy', accuracy_score(y_test_tfidfV_ngram, y_pred))

print('Accuracy of test', tree_model_tf_n.score(X_test_tfidfV_ngram, y_test_tfidfV_ngram))

plott(y_test_tfidfV_ngram, y_pred)

"""tfidf without ngrams"""

tree_model_tf = DecisionTreeClassifier(random_state=0)
tree_model_tf.fit(X_train_tfidfV, y_train_tfidfV)

scores = cross_val_score(tree_model_tf, X_train_tfidfV, y_train_tfidfV, cv= 10)
print('Accuracy of cross validation: ' , scores.mean())

y_pred = tree_model_tf.predict(X_test_tfidfV)

#print('Accuracy', accuracy_score(y_test_tfidfV, y_pred))

print('Accuracy of test', tree_model_tf.score(X_test_tfidfV, y_test_tfidfV))

plott(y_test_tfidfV, y_pred)

"""# **K-nearest neighbor model**

KNN with count vectorization and N-Gram
"""

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors= 3, weights='distance')
neigh.fit(X_train_countV, y_train_countV)
print('cross validation accuracy: ', cross_val_score(neigh, X_train_countV, y_train_countV, cv=10).mean())
predicted = neigh.predict(X_test_countV)
print ('accuracy of test: {}'.format(np.mean(predicted == y_test_countV)))

plott(y_test_countV,predicted)

"""KNN with TF-IDF"""

neigh = KNeighborsClassifier(n_neighbors= 3, weights='distance')
neigh.fit(X_train_tfidfV, y_train_tfidfV)
print('cross validation accuracy: ', cross_val_score(neigh, X_train_tfidfV, y_train_tfidfV, cv=10).mean())
predicted = neigh.predict(X_test_tfidfV)
print ('accuracy of test: {}'.format(np.mean(predicted == y_test_tfidfV)))

plott(y_test_tfidfV,predicted)

"""KNN with TF-IDF/N-Gram"""

neigh = KNeighborsClassifier(n_neighbors=3, weights='distance')
neigh.fit(X_train_tfidfV_ngram, y_train_tfidfV_ngram)
print('Cross validation accuracy: ', cross_val_score(neigh, X_train_tfidfV_ngram, y_train_tfidfV_ngram, cv=10).mean())
predicted = neigh.predict(X_test_tfidfV_ngram)
print ('accuracy of test: {}'.format(np.mean(predicted == y_test_tfidfV_ngram)))

plott(y_test_tfidfV_ngram,predicted)

"""KNN with count Vectorizer

"""

neigh = KNeighborsClassifier(n_neighbors=3, weights='distance', p=1)
neigh.fit(X_train_cv, y_train_cv)
print('cross validation accuracy: ', cross_val_score(neigh, X_train_cv, y_train_cv, cv=10).mean())
predicted = neigh.predict(X_test_cv)
print ('accuracy of test: {}'.format(np.mean(predicted == y_test_cv)))

plott(y_test_cv,predicted)

"""# Decrease model by 20%  Support vector machine Classification - TF-IDF/N-
#From 100 word to 20 word
 
"""

dataFrameT = pd.DataFrame()
def stringOrList1(x):
  if type(x) == str:
   textDataPartitioning(x)

  elif type(x) == list:
    for i in range(len(x)):
      textDataPartitioning1(x[i])

def textDataPartitioning1(x): 
 text = nltk.corpus.gutenberg.raw(x)
 stop_words = set(stopwords.words('english')) 

 text = re.sub(r'[^\w\s]', '', str(text).lower().strip())
 text = re.sub(r'[\d_]', '', text)

 print("--- removing stop words ---")           #stop words must be removed first
 word_tokens = word_tokenize(text)
 filtered_words = [] 
 for w in word_tokens: 
     if w not in stop_words: 
         filtered_words.append(w)

#  print("--- stemming ---")                       #stemming is supposed to show better performance than lemmatization but we will test both
#  new_text = stemming_text(filtered_words)

 print("--- lemmatization ---") 
 new_text = lemm_text(filtered_words)

 filtered_sentence = (new_text.strip()).split()

 temp = []
 list_of_lists = []
 list_of_lists1 = []
 index=0

 for word in filtered_sentence:
   temp.append(word)
   if len(temp) == 20:
    list_of_lists.append(temp)
    temp=[]

 lenght = 0

 for i in range(200):
  ran = random.randint(0, len(list_of_lists)-1)
  list_of_lists1.append([listToString(list_of_lists[ran]), x])

 dataFrame = pd.DataFrame(list_of_lists1, columns=["partition", "book"])
 global dataFrameT  
 dataFrameT = dataFrameT.append(dataFrame, ignore_index = True)


poetry_genre = ['blake-poems.txt', 'milton-paradise.txt', 'whitman-leaves.txt']
fiction_genre = ['burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-thursday.txt', 'melville-moby_dick.txt']
stringOrList1(fiction_genre)
dataFrameT['word_count'] = dataFrameT["partition"].apply(lambda x: len(str(x).split(" ")))
dataFrameT['char_count'] = dataFrameT["partition"].apply(lambda x: sum(len(word) for word in str(x).split(" ")))
dataFrameT['sentence_count'] = dataFrameT["partition"].apply(lambda x: len(str(x).split(".")))
dataFrameT['avg_word_length'] = dataFrameT['char_count'] / dataFrameT['word_count']
dataFrameT['avg_sentence_lenght'] = dataFrameT['word_count'] / dataFrameT['sentence_count']
dataFrameT['Author'] = dataFrameT['book'].apply(lambda x: x.split('-')[0])
from textblob import TextBlob
dataFrameT["Sentiment"] = dataFrameT["partition"].apply(lambda x: 
                   TextBlob(x).sentiment.polarity)
from sklearn.feature_extraction.text import TfidfVectorizer          # we will use the TFIDF vectorizer with n-grams dataset as input as a second test
tfidf_vectorizer_ngram = TfidfVectorizer(use_idf=True, ngram_range = (1, 2), max_features = 2500) 
X_train_tfidf_ngram = tfidf_vectorizer_ngram.fit_transform(dataFrameT["partition"])
X_train_tfidf_ngram.shape
dataset_tfidfV_ngram = pd.DataFrame(X_train_tfidf_ngram.toarray(), columns = tfidf_vectorizer_ngram.get_feature_names())
dataset_tfidfV_ngram = dataset_tfidfV_ngram.join(sentiment, rsuffix = '_score')
tfidf_vectorizer = TfidfVectorizer(use_idf=True , max_features = 2500)          # we will use the TFIDF vectorizer without n-grmas dataset as input as a third test
X_train_tfidf = tfidf_vectorizer.fit_transform(dataFrameT["partition"])
dataset_tfidfV = pd.DataFrame(X_train_tfidf.toarray(), columns = tfidf_vectorizer.get_feature_names())
dataset_tfidfV = dataset_tfidfV.join(sentiment, rsuffix = '_score')
# first we join the author column
author = dataFrameT["Author"]
dataset_countV = dataset_countV.join(author, rsuffix = "_")

# then we shuffle the data
dataset_countV = dataset_countV.sample(frac = 1)

# then we split the input and the target
dataset_countV_target = dataset_countV['Author']
dataset_countV_input = dataset_countV.drop(columns = ['Author'])

# dataset_countV_target
# first we join the author column
dataset_tfidfV_ngram = dataset_tfidfV_ngram.join(author, rsuffix = "_")

# then we shuffle the data
dataset_tfidfV_ngram = dataset_tfidfV_ngram.sample(frac = 1)

# then we split the input and the target
dataset_tfidfV_ngram_target = dataset_tfidfV_ngram['Author']
dataset_tfidfV_ngram_input = dataset_tfidfV_ngram.drop(columns = ['Author'])

# dataset_tfidfV_ngram_target



from sklearn.model_selection import train_test_split


#tfidf with ngrams
X_train_tfidfV_ngram, X_test_tfidfV_ngram, y_train_tfidfV_ngram, y_test_tfidfV_ngram = train_test_split(dataset_tfidfV_ngram_input, dataset_tfidfV_ngram_target, test_size=0.4, random_state=0)

clf_dropped = SVC(kernel= 'linear').fit(X_train_tfidfV_ngram, y_train_tfidfV_ngram)
print('cross validation accuracy: ', cross_val_score(clf_dropped, X_train_tfidfV_ngram, y_train_tfidfV_ngram, cv=10).mean())
predicted1 = clf_dropped.predict(X_test_tfidfV_ngram)
print ('accuracy of test: {}'.format(clf_dropped.score(X_test_tfidfV_ngram, y_test_tfidfV_ngram)))
plott(predicted1,y_test_tfidfV_ngram)

# get a list of the wrong predictions
wrong_pred = []
right_pred = []
wrong_features = []
wrong_feat = []
wrong_feat_dict = {}
wrong_pred_dict = {}
for i in range(len(predicted1) - 1):
  if predicted1[i] != y_test_tfidfV_ngram.iloc[i]:
    wrong_pred.append(predicted1[i])
    # wrong_features.append([X_test_tfidfV_ngram.loc[i, (X_test_tfidfV_ngram != 0).any(axis=0)]])
    right_pred.append(y_test_tfidfV_ngram.iloc[i])

wrong_pred_dict['Predicted Label'] = wrong_pred
wrong_pred_dict['Correct Label'] = right_pred
wrong_pred_dataframe = pd.DataFrame(wrong_pred_dict, columns = ['Predicted Label', 'Correct Label'])


print(X_test_tfidfV_ngram.shape[1])


wrong_pred_dataframe

# word cloud for melville-moby-dick
melville_corpus = nltk.corpus.gutenberg.raw('melville-moby_dick.txt')
wc = wordcloud.WordCloud(background_color='black', max_words=100, 
                         max_font_size=35)
wc = wc.generate(str(melville_corpus))
fig = plt.figure(num=1)
plt.axis('off')
plt.imshow(wc, cmap=None)
plt.show()

# word cloud for melville-moby-dick
chesterton_corpus = nltk.corpus.gutenberg.raw('chesterton-ball.txt')
wc = wordcloud.WordCloud(background_color='black', max_words=100, 
                         max_font_size=35)
wc = wc.generate(str(chesterton_corpus))
fig = plt.figure(num=1)
plt.axis('off')
plt.imshow(wc, cmap=None)
plt.show()

# word cloud for melville-moby-dick
chesterton_corpus = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')
wc = wordcloud.WordCloud(background_color='black', max_words=100, 
                         max_font_size=35)
wc = wc.generate(str(chesterton_corpus))
fig = plt.figure(num=1)
plt.axis('off')
plt.imshow(wc, cmap=None)
plt.show()

y_train_tfidfV_ngram

from mlxtend.evaluate import bias_variance_decomp

# X_train_tfidfV_ngram, X_test_tfidfV_ngram, y_train_tfidfV_ngram, y_test_tfidfV_ngram = train_test_split(X_train_tfidf_ngram , dataset_tfidfV_ngram_target, test_size=0.4, random_state=0)

# estimate bias and variance
clf_dropped = SVC(kernel= 'linear')
mse, bias, var = bias_variance_decomp(clf_dropped, X_train_tfidfV_ngram.to_numpy(), np.array(string_to_integer(y_train_tfidfV_ngram)), X_test_tfidfV_ngram.to_numpy(), np.array(string_to_integer(y_test_tfidfV_ngram)), loss='mse', num_rounds=200, random_seed=1)
# summarize results
print('MSE: %.3f' % mse)
print('Bias: %.3f' % bias)
print('Variance: %.3f' % var)