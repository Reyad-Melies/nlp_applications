# -*- coding: utf-8 -*-
"""Text Data Partitioning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TzJ2TLtCJVTlFmpdcHNnuFgoEjaTrL2Y
"""

import nltk
nltk.download('gutenberg')
nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
import random
nltk.download('punkt')
import pandas as pd

text=nltk.corpus.gutenberg.raw('chesterton-thursday.txt')
tokenized_sents=nltk.sent_tokenize(text)

#global dataFrameT
dataFrameT = pd.DataFrame()

def listToString(s): 
  str1 = ""   
  for ele in s: 
   str1 +=' '
   str1 += ele  
  return str1
def stringOrList(x):
  if type(x)==str:
   textDataPartitioning(x)
  elif type(x)==list:
    for i in range(len(x)):
      textDataPartitioning(x[i])

def textDataPartitioning(x): 
 example_sent=nltk.corpus.gutenberg.raw(x)
 stop_words = set(stopwords.words('english')) 
 word_tokens = word_tokenize(example_sent)
 filtered_sentence = [] 
 for w in word_tokens: 
     if w not in stop_words: 
         filtered_sentence.append(w) 
 temp=[]
 list_of_lists = []
 list_of_lists1 = []
 index=0
 for word in filtered_sentence:
   temp.append(word)
   if len(temp)==100:
    list_of_lists.append(temp)
    temp=[]
 lenght=0
 for i in range(200):
  ran=random.randint(0, len(list_of_lists)-1)
  list_of_lists1.append([list_of_lists[ran], x])
 dataFrame = pd.DataFrame(list_of_lists1, columns=["partition", "book"])
 global dataFrameT  
 dataFrameT=dataFrameT.append(dataFrame)

#list of 100 words in first column
oo=['chesterton-thursday.txt','austen-emma.txt']
stringOrList(oo)
print(dataFrameT)