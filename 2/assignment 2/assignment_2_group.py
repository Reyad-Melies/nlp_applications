# -*- coding: utf-8 -*-
"""Assignment 2 - group

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GF7NnsiIy5-g4WQv6MX06zR1iNvLBRzA
"""

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
import random
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
import wordcloud
from sklearn import metrics
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, accuracy_score
import warnings
warnings.filterwarnings('ignore')

# from google.colab import drive
# drive.mount('/content/drive')

nltk.download('gutenberg')
nltk.download("stopwords")
nltk.download('punkt')
nltk.download('wordnet')

#global dataFrameT
dataFrameT = pd.DataFrame()

def listToString(s): 
  text = " ".join(s)
  return text

def stringOrList(x):
  if type(x) == str:
   textDataPartitioning(x)

  elif type(x) == list:
    for i in range(len(x)):
      textDataPartitioning(x[i])

def stemming_text(words):
  stemmer = PorterStemmer()
  # words = text.split()
  words = [stemmer.stem(word.strip()) for word in words]
  return " ".join(words)

def lemm_text(words):
  lemmatizer = WordNetLemmatizer()
  # words = text.split()
  words = [lemmatizer.lemmatize(word, pos='v') for word in words]
  return " ".join(words)

def textDataPartitioning(x): 
 text = nltk.corpus.gutenberg.raw(x)
 stop_words = set(stopwords.words('english')) 

 text = re.sub(r'[^\w\s]', '', str(text).lower().strip())
 text = re.sub(r'[\d_]', '', text)

 print("--- removing stop words ---")           #stop words must be removed first
 word_tokens = word_tokenize(text)
 filtered_words = [] 
 for w in word_tokens: 
     if w not in stop_words: 
         filtered_words.append(w)

 print("--- stemming ---")                       #stemming is supposed to show better performance than lemmatization but we will test both
 new_text = stemming_text(filtered_words)

#  print("--- lemmatization ---") 
#  new_text = lemm_text(filtered_words)

 filtered_sentence = (new_text.strip()).split()

 temp = []
 list_of_lists = []
 list_of_lists1 = []
 index=0

 for word in filtered_sentence:
   temp.append(word)
   if len(temp) == 150:
    list_of_lists.append(temp)
    temp=[]

 for i in range(200):
  ran = random.randint(0, len(list_of_lists)-1)
  list_of_lists1.append([listToString(list_of_lists[ran]), x])

 dataFrame = pd.DataFrame(list_of_lists1, columns=["partition", "book"])
 global dataFrameT  
 dataFrameT = dataFrameT.append(dataFrame, ignore_index = True)

book_list = ['bible-kjv.txt', 'milton-paradise.txt', 'chesterton-brown.txt', 'shakespeare-macbeth.txt', 'melville-moby_dick.txt']

stringOrList(book_list)
# dataFrameT.to_csv("dataset.csv", index=False)
dataFrameT

"""# Import Dataset from here <---------

"""

dataFrameT = pd.read_csv('dataset.csv')

DataFrameOrg = dataFrameT.copy()

dataFrameT = DataFrameOrg.copy()   #use this line to get the same data you got before

dataFrameT['word_count'] = dataFrameT["partition"].apply(lambda x: len(str(x).split(" ")))
dataFrameT['char_count'] = dataFrameT["partition"].apply(lambda x: sum(len(word) for word in str(x).split(" ")))
dataFrameT['sentence_count'] = dataFrameT["partition"].apply(lambda x: len(str(x).split(".")))
dataFrameT['avg_word_length'] = dataFrameT['char_count'] / dataFrameT['word_count']
dataFrameT['avg_sentence_lenght'] = dataFrameT['word_count'] / dataFrameT['sentence_count']
dataFrameT['Author'] = dataFrameT['book'].apply(lambda x: x.split('-')[0])
dataFrameT

from textblob import TextBlob
dataFrameT["Sentiment"] = dataFrameT["partition"].apply(lambda x: 
                   TextBlob(x).sentiment.polarity)
dataFrameT

dataFrameT.shape
dataFrameT.loc[:,['partition', 'Author']]

top=10
nltk.download('punkt')
## for vectorizer
from sklearn import feature_extraction, manifold

corpus = dataFrameT["partition"]
lst_tokens = nltk.tokenize.word_tokenize(corpus.str.cat(sep=" "))
fig, ax = plt.subplots(nrows=1, ncols=2)
fig.suptitle("Most frequent words", fontsize=15)
    
## unigrams
dic_words_freq = nltk.FreqDist(lst_tokens)
dtf_uni = pd.DataFrame(dic_words_freq.most_common(), 
                       columns=["Word","Freq"])
dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Unigrams", ax=ax[0], 
                  legend=False).grid(axis='x')
ax[0].set(ylabel=None)
    
## bigrams
dic_words_freq = nltk.FreqDist(nltk.ngrams(lst_tokens, 2))
dtf_bi = pd.DataFrame(dic_words_freq.most_common(), 
                      columns=["Word","Freq"])
dtf_bi["Word"] = dtf_bi["Word"].apply(lambda x: " ".join(
                   string for string in x) )
dtf_bi.set_index("Word").iloc[:top,:].sort_values(by="Freq").plot(
                  kind="barh", title="Bigrams", ax=ax[1],
                  legend=False).grid(axis='x')
ax[1].set(ylabel=None)
plt.show()

"""Visualization:"""

wc = wordcloud.WordCloud(background_color='black', max_words=100, 
                         max_font_size=35)
wc = wc.generate(str(corpus))
fig = plt.figure(num=1)
plt.axis('off')
plt.imshow(wc, cmap=None)
plt.show()

import seaborn as sp
def plott(label,y_pred):
  clf_report = classification_report(label,
                                   y_pred,
                                   target_names=['burgess','carroll','chesterton','melville'],
                                   output_dict=True)
  print('\nConfusion Matrix:\n' )
  print(multilabel_confusion_matrix(label, y_pred))
  sp.heatmap(confusion_matrix(label, y_pred))
  plt.show()
  print('\nClassification Report:\n' )
  print(classification_report(label, y_pred))
  sp.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)
  plt.show()

import matplotlib.pyplot as plt
def funp(y,yp):
  plt.plot(y,'x')
  plt.plot(yp, 'o')
  plt.show()

"""# Tokenization and Transformation

## Count Vectorization (BoW)
"""

from sklearn.feature_extraction.text import CountVectorizer
count_v = CountVectorizer(max_features=2500)
X_train_counts_v = count_v.fit_transform(dataFrameT["partition"])
print(X_train_counts_v.shape)

dataset_CV_bow = pd.DataFrame(X_train_counts_v.toarray(), columns = count_v.get_feature_names())
sentiment = pd.DataFrame(data = dataFrameT.loc[:,"Sentiment"], columns = ['Sentiment'])
# dataset_CV_bow = dataset_CV_bow.join(sentiment, rsuffix = '_score')
dataset_CV_bow

"""# TFIDF Vectorization"""

from sklearn.feature_extraction.text import TfidfVectorizer 
tfidf_vectorizer = TfidfVectorizer(use_idf=True, max_features=2500)          # we will use the TFIDF vectorizer without n-grmas dataset as input as a third test
X_train_tfidf = tfidf_vectorizer.fit_transform(dataFrameT["partition"])
X_train_tfidf.shape

dataset_tfidfV = pd.DataFrame(X_train_tfidf.toarray(), columns = tfidf_vectorizer.get_feature_names())
# dataset_tfidfV = dataset_tfidfV.join(sentiment, rsuffix = '_score')
dataset_tfidfV

"""# Word Embeddings"""

import gensim.downloader as gensim_api
import gensim
from gensim.models import Word2Vec

dataframe_we = pd.DataFrame()
dataframe_we['partition'] = dataFrameT['partition']

untokenized_data = dataframe_we['partition'].tolist()
tokenized_data = []
for doc in untokenized_data:
  tokenized_data.append(doc.split())

word_model = Word2Vec(tokenized_data, min_count = 1, seed = 123)

print(word_model)

def vectorize(list_of_docs, model):
    """Generate vectors for list of documents using a Word Embedding

    Args:
        list_of_docs: List of documents
        model: Gensim's Word Embedding

    Returns:
        List of document vectors
    """
    features = []

    for tokens in list_of_docs:
        zero_vector = np.zeros(model.vector_size)
        vectors = []
        for token in tokens:
            if token in model.wv:
                try:
                    vectors.append(model.wv[token])
                except KeyError:
                    continue
        if vectors:
            vectors = np.asarray(vectors)
            avg_vec = vectors.mean(axis=0)
            features.append(avg_vec)
        else:
            features.append(zero_vector)
    return features

vectorized_docs = vectorize(tokenized_data, model=word_model)
len(vectorized_docs), len(vectorized_docs[0])
# vectorized_docs
dataframe_we = pd.DataFrame(data = vectorized_docs)
dataframe_we

from sklearn.decomposition import PCA
from matplotlib import pyplot

# X = word_model[word_model.wv.vocab]
pca = PCA(n_components=2)
result = pca.fit_transform(dataframe_we)
# create a scatter plot of the projection
pyplot.scatter(result[:, 0], result[:, 1])
words = list(word_model.wv.vocab)
# for i, word in enumerate(words):
# 	pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))
pyplot.show()

"""# Doc2Vec"""

from gensim.models.doc2vec import Doc2Vec, TaggedDocument

tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_data)]

## Train doc2vec model
doc2vec_model = Doc2Vec(tagged_data, vector_size=20, window=2, min_count=1, workers=4, epochs = 100)

# doc2vec_model.wv.vocab

test_doc = tokenized_data[1]
doc2vec_model.docvecs.most_similar(positive=[doc2vec_model.infer_vector(test_doc)],topn=5)

from scipy import spatial
vec1 = doc2vec_model.infer_vector(tokenized_data[0])
vec2 = doc2vec_model.infer_vector(tokenized_data[300])

cos_distance = spatial.distance.cosine(vec1, vec2)
print("cosine distance: ", cos_distance)

dict_dist = {}
list_row = []

# for i in range(len(tokenized_data)):
#   for j in range(len(tokenized_data)):
#     vec1 = doc2vec_model.infer_vector(tokenized_data[i])
#     vec2 = doc2vec_model.infer_vector(tokenized_data[j])
#     list_row.append(spatial.distance.cosine(vec1, vec2))
#   dict_dist[i] = list_row
#   list_row = []

# dataframe_doc2vec = pd.DataFrame(data = dict_dist)

"""# LDA"""

# !pip install pyLDAvis

# !pip install pandas==1.2.0

import gensim
from gensim import corpora
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
from gensim.models.coherencemodel import CoherenceModel

dataframe_lda = pd.DataFrame()
dataframe_lda['partition'] = dataFrameT['partition']

untokenized_data = dataframe_lda['partition'].tolist()
tokenized_data = []
for doc in untokenized_data:
  tokenized_data.append(doc.split())

print(len(tokenized_data))
dictionary = corpora.Dictionary(tokenized_data)
doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_data]

def compute_coherence_preplexity_values(dictionary, corpus, texts, limit, start=2, step=3):
    """
    Compute c_v coherence for various number of topics

    Parameters:
    ----------
    dictionary : Gensim dictionary
    corpus : Gensim corpus
    texts : List of input texts
    limit : Max num of topics

    Returns:
    -------
    model_list : List of LDA topic models
    coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    """
    coherence_values = []
    
    model_list = []
    for num_topics in range(start, limit, step):
        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, alpha = 0.5)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())
        

    return model_list, coherence_values

import warnings
warnings.filterwarnings('ignore')
model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=doc_term_matrix, texts=tokenized_data, start=2, limit=50, step=1)

# Show graph
limit=50; start=2; step=1;
x = range(start, limit, step)
plt.plot(x, coherence_values)
plt.xlabel("Num Topics")
plt.ylabel("Coherence score")
plt.legend(("coherence_values"), loc='best')
plt.show()# Print the coherence scores

print("\nHighest Coherence is at num topics: ", x[coherence_values.index(max(coherence_values))])

# the optimal model is at 2 topics
optimal_model = model_list[x[coherence_values.index(max(coherence_values))] - 2]
pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(optimal_model, doc_term_matrix, dictionary)
vis

"""# Shuffling data

### count vectorizer (BoW)
"""

# first we join the author column
author = dataFrameT["Author"]
dataset_CV_bow = dataset_CV_bow.join(author, rsuffix = "_")

# then we shuffle the data
dataset_CV_bow = dataset_CV_bow.sample(frac = 1)

# then we split the input and the target
dataset_CV_bow_target = dataset_CV_bow['Author']
dataset_CV_bow_input = dataset_CV_bow.drop(columns = ['Author'])

# dataset_countV_target

"""### TFIDF"""

# first we join the author column
dataset_tfidfV = dataset_tfidfV.join(author, rsuffix = "_")

# then we shuffle the data
dataset_tfidfV = dataset_tfidfV.sample(frac = 1)

# then we split the input and the target
dataset_tfidfV_target = dataset_tfidfV['Author']
dataset_tfidfV_input = dataset_tfidfV.drop(columns = ['Author'])

# dataset_tfidfV_input

dataset_tfidfV.to_csv("tfidf_data.csv", index=False)
dataset_CV_bow.to_csv("bow_data.csv", index=False)

"""# Run from this point <<<------

"""

dataset_tfidfV = pd.read_csv("tfidf_data.csv")
dataset_CV_bow = pd.read_csv("bow_data.csv")

dataset_tfidfV_target = dataset_tfidfV['Author']
dataset_tfidfV_input = dataset_tfidfV.drop(columns = ['Author'])
dataset_tfidfV_input = dataset_tfidfV_input.iloc[:,1:]

dataset_CV_bow_target = dataset_CV_bow['Author']
dataset_CV_bow_input = dataset_CV_bow.drop(columns = ['Author'])
dataset_CV_bow_input = dataset_CV_bow_input.iloc[:,1:]

dataset_CV_bow_input

>>> from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
dataset_tfidfV_input = scaler.fit_transform(dataset_tfidfV_input)
dataset_CV_bow_input = scaler.fit_transform(dataset_CV_bow_input)

"""# splitting the data

# ----> No splitting use the original datasets <-----

# K-means
"""

silhouette_scores = []
kappa_scores = []
model_names = []

from collections import Counter
import operator

def get_real_labels(predicted_labels, real_labels, no_clusters):
  # get unique predicted_labels
  unique_list_predicted = list(range(no_clusters))  
  #   # traverse for all elements
  # for x in predicted_labels:
  #     # check if exists in unique_list or not
  #     if x not in unique_list_predicted:
  #         unique_list_predicted.append(x)
  
  # get unique real_labels
  unique_list_real = []  
    # traverse for all elements
  for x in real_labels:
      # check if exists in unique_list or not
      if x not in unique_list_real:
          unique_list_real.append(x)
  
  dict_output = {}

  for j in unique_list_real:
    dict_output[j] = []
    for i in range(len(predicted_labels)):
      if real_labels[i] == j:
        dict_output[j].append(predicted_labels[i])

# not the dictionary has all classes with predicted clusters

  dict_counter = {}

  for j in unique_list_real:
    dict_counter[j] = {}
    for i in range(no_clusters):
      dict_counter[j][i] = 0
      for l in range(len(dict_output[j])):
        if list(dict_output[j])[l] == i:
          dict_counter[j][i] = dict_counter[j][i] + 1

  dict_label_cluster = {}


  for j in unique_list_real:
    sorted_count = dict_counter[j]     #Counter(dict_output[j])   #key: cluster no, value: count
    sorted_count = dict(sorted(sorted_count.items(), key=operator.itemgetter(1), reverse=True))
    # print(sorted_count)
    for i in range(len(list(sorted_count.keys()))):
      # print(list(dict_label_cluster.keys()))
      # print(list(sorted_count.keys()))
      if list(sorted_count.keys())[i] in list(dict_label_cluster.values()):
        # print('yes')
        pass
      elif list(sorted_count.keys())[i] not in list(dict_label_cluster.values()):
        # print('no')
        dict_label_cluster[j] = list(sorted_count.keys())[i]
        break
      else:
        for k in unique_list_predicted:
          if k not in list(dict_label_cluster.values()):
            # print('here')
            dict_label_cluster[j] = k
            break
        break


  return dict_label_cluster

from sklearn.cluster import KMeans
from sklearn.manifold import TSNE
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics.cluster import homogeneity_score,completeness_score,adjusted_rand_score,adjusted_mutual_info_score
#from sklearn.metrics.cluster import 
from sklearn.metrics import silhouette_samples, silhouette_score,v_measure_score,cohen_kappa_score
from scipy import stats

import seaborn as sp
def plott(label,y_pred):
  clf_report = classification_report(label,
                                   y_pred,
                                   target_names=['bible-kjv.txt', 'milton-paradise.txt', 'chesterton-brown.txt', 'shakespeare-macbeth.txt', 'melville-moby_dick.txt'],
                                   output_dict=True)
  print('\nConfusion Matrix:\n' )
  print(multilabel_confusion_matrix(label, y_pred))
  sp.heatmap(confusion_matrix(label, y_pred))
  plt.show()
  print('\nClassification Report:\n' )
  print(classification_report(label, y_pred))
  sp.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)
  plt.show()

def metrics(dataset, labels, clusters):
  print("cohen_kappa score: ",cohen_kappa_score(labels, clusters))
  print("homogeneity score: ", homogeneity_score(labels, clusters))
  # print("completeness score: ", completeness_score(labels, clusters))
  # print("v_measure score: ", v_measure_score(labels, clusters))
  # print("adjusted_rand_score: ", adjusted_rand_score(labels, clusters))
  # print("adjusted_mutual_info score: ", adjusted_mutual_info_score(labels,  clusters))
  # print("cohen_kappa linear score: ", cohen_kappa_score(labels, clusters,weights='linear'))
  # print("spearman: ", stats.spearmanr(labels,clusters))
  print("Silhouette score: ", silhouette_score(dataset, clusters))

  return cohen_kappa_score(labels, clusters), silhouette_score(dataset, clusters)

from sklearn import preprocessing
def col(y_train_tfidfV):
  label_ytrain = y_train_tfidfV
  le = preprocessing.LabelEncoder()
  le.fit(label_ytrain)
  label_ytrain = le.transform(label_ytrain)
  return label_ytrain

reduced_data = PCA(2).fit_transform(dataset_tfidfV_input)
label_ytrain = col(dataset_tfidfV_target)
plt.scatter(reduced_data[:,0], reduced_data[:,1], c= label_ytrain)
plt.show()

#cohen_kappa_score(pred_y, y_test_cv)
kmeans = KMeans(n_clusters=5, init='k-means++').fit(dataset_tfidfV_input)
pred_y = kmeans.fit_predict(dataset_tfidfV_input)
reduced_data = PCA(2).fit_transform(dataset_tfidfV_input)



label_ytrain_tfidf = col(pred_y)
label_ytrain1_tfidf = col(dataset_tfidfV_target)


plt.scatter(reduced_data[:,0], reduced_data[:,1], c= label_ytrain1_tfidf)
plt.show()
plt.scatter(reduced_data[:,0], reduced_data[:,1], c= label_ytrain_tfidf)
#plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

# # t-SNE plot
# embeddings = TSNE(2).fit_transform(dataset_tfidfV_input)
# plt.scatter(embeddings[:,0], embeddings[:,1], c= label_ytrain1_tfidf)
# plt.show()
# plt.scatter(embeddings[:,0], embeddings[:,1], c= label_ytrain_tfidf)
# plt.show()

dictionary = get_real_labels(label_ytrain_tfidf, dataset_tfidfV_target, 5)
print(dictionary)
numArray = []
for item in dataset_tfidfV_target:
    numArray.append(dictionary.get(item))

metrics(dataset_tfidfV_input, label_ytrain_tfidf, numArray)
silhouette_scores.append(metrics(dataset_tfidfV_input, label_ytrain_tfidf, numArray)[1])
kappa_scores.append(metrics(dataset_tfidfV_input, label_ytrain_tfidf, numArray)[0])
model_names.append('K-means with TFIDF and k = 5')
# plott(label_ytrain1,label_ytrain)

"""# Using PCA before clustering"""

trainn = PCA(2).fit_transform(dataset_tfidfV_input)
kmeans_w = KMeans(n_clusters=5, init='k-means++').fit(trainn)
XX=PCA(2).fit_transform(dataset_tfidfV_input)
predictt=kmeans_w.fit_predict(XX)


label_ytrain_tfidf2 = col(predictt)
label_ytrain1_tfidf2 = col(dataset_tfidfV_target)


plt.scatter(XX[:,0], XX[:,1], c= label_ytrain1_tfidf2)
plt.show()
plt.scatter(XX[:,0], XX[:,1], c= label_ytrain_tfidf2)
#plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

dictionary2 = get_real_labels(label_ytrain_tfidf2, dataset_tfidfV_target, 5)
print(dictionary2)
numArray2 = []
for item in dataset_tfidfV_target:
    numArray2.append(dictionary2.get(item))


metrics(dataset_tfidfV_input, label_ytrain_tfidf2, numArray2)
silhouette_scores.append(metrics(dataset_tfidfV_input, label_ytrain_tfidf2, numArray2)[1])
kappa_scores.append(metrics(dataset_tfidfV_input, label_ytrain_tfidf2, numArray2)[0])
model_names.append('K-means with TFIDF and k = 5 and PCA')
# plott(label_ytrain1,label_ytrain)

trainn = TSNE(2).fit_transform(dataset_tfidfV_input)
kmeans = KMeans(n_clusters=5, init='k-means++').fit(trainn)
XX=TSNE(2).fit_transform(dataset_tfidfV_input)
predictt=kmeans.fit_predict(XX)


label_ytrain_tfidf3 = col(predictt)
label_ytrain1_tfidf3 = col(dataset_tfidfV_target)


plt.scatter(XX[:,0], XX[:,1], c= label_ytrain1_tfidf3)
plt.show()
plt.scatter(XX[:,0], XX[:,1], c= label_ytrain_tfidf3)
#plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

dictionary3 = get_real_labels(label_ytrain_tfidf3, dataset_tfidfV_target, 5)
print(dictionary3)
numArray3 = []
for item in dataset_tfidfV_target:
    numArray3.append(dictionary3.get(item))


metrics(dataset_tfidfV_input, label_ytrain_tfidf3, numArray3)
silhouette_scores.append(metrics(dataset_tfidfV_input, label_ytrain_tfidf3, numArray3)[1])
kappa_scores.append(metrics(dataset_tfidfV_input, label_ytrain_tfidf3, numArray3)[0])
model_names.append('K-means with TFIDF and k = 5 and TSNE')
# plott(label_ytrain1,label_ytrain)











reduced_data = PCA(2).fit_transform(dataset_CV_bow_input)
label_ytrain = col(dataset_CV_bow_target)
plt.scatter(reduced_data[:,0], reduced_data[:,1], c= label_ytrain)
plt.show()

#cohen_kappa_score(pred_y, y_test_cv)
kmeans = KMeans(n_clusters=5, init='k-means++').fit(dataset_CV_bow_input)
pred_y = kmeans.fit_predict(dataset_CV_bow_input)
reduced_data = PCA(2).fit_transform(dataset_CV_bow_input)



label_ytrain_cv = col(pred_y)
label_ytrain1_cv = col(dataset_CV_bow_target)


plt.scatter(reduced_data[:,0], reduced_data[:,1], c= label_ytrain1_cv)
plt.show()
plt.scatter(reduced_data[:,0], reduced_data[:,1], c= label_ytrain_cv)
#plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

# t-SNE plot
embeddings = TSNE(2).fit_transform(dataset_CV_bow_input)
plt.scatter(embeddings[:,0], embeddings[:,1], c= label_ytrain1_cv)
plt.show()
plt.scatter(embeddings[:,0], embeddings[:,1], c= label_ytrain_cv)
plt.show()

dictionary4 = get_real_labels(label_ytrain_cv, dataset_CV_bow_target, 5)
print(dictionary4)
numArray4 = []
for item in dataset_CV_bow_target:
    numArray4.append(dictionary4.get(item))

metrics(dataset_CV_bow_input, label_ytrain_cv, numArray4)
silhouette_scores.append(metrics(dataset_CV_bow_input, label_ytrain_cv, numArray4)[1])
kappa_scores.append(metrics(dataset_CV_bow_input, label_ytrain_cv, numArray4)[0])
model_names.append('K-means with BOW and k = 5')
# plott(label_ytrain1,label_ytrain)

trainn = PCA(2).fit_transform(dataset_CV_bow_input)
kmeans = KMeans(n_clusters=5, init='k-means++').fit(trainn)
XX=PCA(2).fit_transform(dataset_CV_bow_input)
predictt=kmeans.fit_predict(XX)


label_ytrain_cv2 = col(predictt)
label_ytrain1_cv2 = col(dataset_CV_bow_target)


plt.scatter(XX[:,0], XX[:,1], c= label_ytrain1_cv2)
plt.show()
plt.scatter(XX[:,0], XX[:,1], c= label_ytrain_cv2)
#plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

dictionary5 = get_real_labels(label_ytrain_cv2, dataset_CV_bow_target, 5)
print(dictionary5)
numArray5 = []
for item in dataset_CV_bow_target:
    numArray5.append(dictionary5.get(item))

metrics(dataset_CV_bow_input, label_ytrain_cv2, numArray5)
silhouette_scores.append(metrics(dataset_CV_bow_input, label_ytrain_cv2, numArray5)[1])
kappa_scores.append(metrics(dataset_CV_bow_input, label_ytrain_cv2, numArray5)[0])
model_names.append('K-means with BOW and k = 5 and PCA')
# plott(label_ytrain1,label_ytrain)

trainn = TSNE(2).fit_transform(dataset_CV_bow_input)
kmeans = KMeans(n_clusters=5, init='k-means++').fit(trainn)
XX=TSNE(2).fit_transform(dataset_CV_bow_input)
predictt=kmeans.fit_predict(XX)


label_ytrain_cv3 = col(predictt)
label_ytrain1_cv3 = col(dataset_CV_bow_target)


plt.scatter(XX[:,0], XX[:,1], c= label_ytrain1_cv3)
plt.show()
plt.scatter(XX[:,0], XX[:,1], c= label_ytrain_cv3)
#plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

dictionary6 = get_real_labels(label_ytrain_cv2, dataset_CV_bow_target, 5)
print(dictionary6)
numArray6 = []
for item in dataset_CV_bow_target:
    numArray6.append(dictionary6.get(item))


metrics(dataset_CV_bow_input, label_ytrain_cv3, numArray6)
silhouette_scores.append(metrics(dataset_CV_bow_input, label_ytrain_cv3, numArray6)[1])
kappa_scores.append(metrics(dataset_CV_bow_input, label_ytrain_cv3, numArray6)[0])
model_names.append('K-means with BOW and k = 5 and TSNE')
# plott(label_ytrain1,label_ytrain)

"""# Expectation Maximization"""

def Author_to_num (data):
  uni_data = data.unique()
  data_dictionary = dict()
  num = 0
  for name in uni_data:
    data_dictionary[name] = num
    num += 1
  
  numArray = []
  for item in data:
    numArray.append(data_dictionary.get(item))
  return numArray

from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture

# reduce the features to 2D
pca = PCA(n_components=2, random_state=42)
reduced_features = pca.fit_transform(dataset_tfidfV_input)

plt.scatter(reduced_features[:,0], reduced_features[:,1], c=Author_to_num(dataset_tfidfV_target))

# distortions = []
# K = range(1,10)
# for k in K:
#     GMM = GaussianMixture(n_components= k, init_params= 'random', random_state= 42,covariance_type= 'spherical').fit(dataset_CV_bow_input)
#     distortions.append(GMM.aic(dataset_CV_bow_input))

# plt.figure(figsize=(16,8))
# plt.plot(K, distortions, 'bx-')
# plt.xlabel('k')
# plt.ylabel('AIC')
# plt.title('The Elbow Method showing the optimal k')
# plt.show()

# reduce the features to 2D
pca = PCA(n_components=2, random_state=42)
reduced_features = pca.fit_transform(dataset_tfidfV_input)

plt.scatter(reduced_features[:,0], reduced_features[:,1], c=Author_to_num(dataset_tfidfV_target))

clf = GaussianMixture(n_components= 5, init_params= 'random', random_state= 42,covariance_type= 'spherical').fit(dataset_tfidfV_input)

predicted1 = clf.predict(dataset_tfidfV_input)

# Forming a Cross tabular of the true labels and the predicted ones
df = pd.DataFrame({'labels': predicted1, 'Authors': dataset_tfidfV_target})
ct = pd.crosstab(df['labels'], df['Authors'])
print(ct)

dictionary = get_real_labels(predicted1, dataset_tfidfV_target, 5)
print(dictionary)
numArray = []
for item in dataset_tfidfV_target:
    numArray.append(dictionary.get(item))

wrong_ones = []
for i in range(0,len(predicted1)):
  if predicted1[i] != numArray[i]:
    wrong_ones.append(i)

k12 = cohen_kappa_score(numArray, predicted1)
s12 = silhouette_score(dataset_tfidfV_input, predicted1)

silhouette_scores.append(s12)
kappa_scores.append(k12)
model_names.append('EM with TFIDF and 5 cluster')

print(len(wrong_ones))
print("The kappa value is {}".format(k12))
print("The Silhouttee value is {}".format(s12))

# Cross tabulation plotting
ct.plot.bar(stacked=True)
plt.show()

# plotting the predicted clusters
plt.scatter(reduced_features[:,0], reduced_features[:,1], c=predicted1)

# distortions = []
# K = range(1,10)
# for k in K:
#     GMM = GaussianMixture(n_components= k, init_params= 'random', random_state= 42,covariance_type= 'spherical').fit(dataset_CV_bow_input)
#     distortions.append(GMM.aic(dataset_CV_bow_input))

# plt.figure(figsize=(16,8))
# plt.plot(K, distortions, 'bx-')
# plt.xlabel('k')
# plt.ylabel('AIC')
# plt.title('The Elbow Method showing the optimal k')
# plt.show()

# plotting the data with the count-vectorier modification
pca = PCA(n_components=2, random_state=42)
reduced_features = pca.fit_transform(dataset_CV_bow_input)

plt.scatter(reduced_features[:,0], reduced_features[:,1], c=Author_to_num(dataset_CV_bow_target))

# Training the data
clf2 = GaussianMixture(n_components= 5, init_params= 'random', random_state= 42,covariance_type= 'spherical').fit(dataset_CV_bow_input)

predicted = clf2.predict(dataset_CV_bow_input)
# Forming a Cross tabular of the true labels and the predicted ones
df = pd.DataFrame({'labels': predicted, 'Authors': dataset_CV_bow_target})
ct2 = pd.crosstab(df['labels'], df['Authors'])
print(ct2)

dictionary = get_real_labels(predicted, dataset_CV_bow_target, 5)
numArray = []
for item in dataset_CV_bow_target:
    numArray.append(dictionary.get(item))

wrong_ones = []
for i in range(0,len(predicted)):
  if predicted[i] != numArray[i]:
    wrong_ones.append(i)

k11 = cohen_kappa_score(numArray, predicted)
s11 = silhouette_score(dataset_CV_bow_input, predicted)

silhouette_scores.append(s11)
kappa_scores.append(k11)
model_names.append('EM with BOW and 5 cluster')

print(len(wrong_ones))
print(wrong_ones)
print("The kappa value is {}".format(k11))
print("The Silhouttee value is {}".format(s11))

# Cross tabulation plotting
ct2.plot.bar(stacked=True)
plt.show()

# plotting the predicted clusters
plt.scatter(reduced_features[:,0], reduced_features[:,1], c=predicted)

"""# Heirarcial Clustering"""

import scipy.cluster.hierarchy as sch
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn import preprocessing
from sklearn.metrics import homogeneity_score, silhouette_score
from sklearn.metrics import cohen_kappa_score

import seaborn as sns

#bible, melville, milton, chesterton, shakespeare
np.unique(dataset_tfidfV_target, return_counts=True)

#tfidfV
pca = PCA(n_components=2)
reduced_features = pca.fit_transform(dataset_tfidfV_input)
len(reduced_features)

def preprocess(y):
  label_y = np.array(y)
  le = preprocessing.LabelEncoder()
  le.fit(label_y)
  label_y = le.transform(label_y)
  return label_y

label_y = preprocess(dataset_tfidfV_target)
plt.scatter(reduced_features[:,0], reduced_features[:,1], c= label_y)

# create dendrogram
plt.figure(figsize=(20, 7))  
plt.title("Dendrograms")  
dendrogram = sch.dendrogram(sch.linkage(dataset_tfidfV_input, method='ward'))
plt.xlabel('Sample index')
plt.ylabel('Distance')

def evaluate_model(dataset, label_y, y):
  h = homogeneity_score(label_y, y)
  s = silhouette_score(dataset, y)
  k = cohen_kappa_score(label_y, y)
  print('homogeneity_score', h)
  print('silhouette_score', s)
  print('cohen_kappa_score', k)
  #print('accuracy_score', accuracy_score(label_y, y))
  
  # print('completeness_score', completeness_score(label_y, y))
  # print('v_measure_score', v_measure_score(label_y, y))
  # print('adjusted_rand_score', adjusted_rand_score(label_y, y))
  # print('adjusted_mutual_info_score', adjusted_mutual_info_score(label_y, y))

  return k, s

def train_AgglomerativeClustering(x, y, distance, linkage):
  hc = AgglomerativeClustering(n_clusters= 5, affinity = distance, linkage = linkage)
  y_hc = hc.fit_predict(x)
  #print(y_hc)
  # label_y = preprocess(y)
  dictionary = get_real_labels(y_hc, y, 5)
  print(dictionary)
  numArray = []
  for item in y:
    numArray.append(dictionary.get(item))

  cohen_kappa_score1, silhouette_score1 = evaluate_model(x, numArray, y_hc)
  # plt.scatter(reduced_features[:,0], reduced_features[:,1], c= y_hc, cmap='viridis')
  # plt.show()
  sns.scatterplot(reduced_features[:,0], reduced_features[:,1], hue= y_hc, palette= 'tab10')

  return cohen_kappa_score1, silhouette_score1

# create clusters with euclidean distance and linkage ward
cohen_kappa_score1, silhouette_score1 = train_AgglomerativeClustering(dataset_tfidfV_input, dataset_tfidfV_target, 'euclidean', 'ward')
silhouette_scores.append(silhouette_score1)
kappa_scores.append(cohen_kappa_score1)
model_names.append("HC 5 clusters TFIDF")

#cv
cohen_kappa_score2, silhouette_score2 = train_AgglomerativeClustering(dataset_CV_bow_input, dataset_CV_bow_target, 'euclidean', 'ward')
silhouette_scores.append(silhouette_score2)
kappa_scores.append(cohen_kappa_score2)
model_names.append("HC\n5 clusters\nBOW")

"""# Choosing the Champion model"""

# we can plot the values of silhouette and kappa score againt the models
plt.figure(figsize=(10, 7)) 
x = list(range(len(model_names)))
plt.plot(x, silhouette_scores)
plt.plot(x, kappa_scores)

plt.show()

print('highest kappa and highest silhouette are with the model: ', model_names[1])
print('silhouette score: ', silhouette_scores[1])
print('Kappa score: ', kappa_scores[1])

"""# Extracitng the top 10 words from wrongly classified data"""

# getting wrongly classified data from the champion model

wrong_ones = pd.DataFrame(columns=['index_wrong', "false_cluster"])
for i in range(0,len(label_ytrain_tfidf2)):
  if predicted[i] != numArray2[i]:
    new_row = {'index_wrong': i, 'false_cluster': label_ytrain_tfidf2[i]}
    wrong_ones = wrong_ones.append(new_row, ignore_index= True)

# Forming a Cross tabular of the true labels and the predicted ones
df = pd.DataFrame({'labels': label_ytrain_tfidf2, 'Authors': dataset_tfidfV_target})
ct22 = pd.crosstab(df['labels'], df['Authors'])
print(ct22)

# Cross tabulation plotting
plt.figure(figsize=(15, 10)) 
ct22.plot.bar(stacked=True)
plt.show()

corpus = dataFrameT["partition"]
for i in wrong_ones['false_cluster'].unique():
  sub_data = wrong_ones[wrong_ones['false_cluster'] == i]
  data = []
  for j in sub_data['index_wrong']:
    data.append(corpus[j])
  data = pd.Series(data)
  lst_tokens = nltk.tokenize.word_tokenize(data.str.cat(sep=" "))
    
  ## unigrams
  dic_words_freq = nltk.FreqDist(lst_tokens)
  dtf_uni = pd.DataFrame(dic_words_freq.most_common(10), 
                       columns=["Word","Freq"])
  dtf_uni.set_index("Word").iloc[:top,:].sort_values(by="Freq")

  print('Most frequent words in cluster {}'.format(i))
  print(dtf_uni)

"""# Comparing some documents with doc2vec

"""

from scipy import spatial

dict_dist = {}
list_row = []
counter = 0

# bible and moby dick

for i in range(30):
  for j in range(900,930):
    vec1 = doc2vec_model.infer_vector(tokenized_data[i])
    vec2 = doc2vec_model.infer_vector(tokenized_data[j])
    spatial1 = spatial.distance.cosine(vec1, vec2)
    if spatial1 > 0.7:
      counter = counter + 1
    list_row.append(spatial1)
  dict_dist[i] = list_row
  list_row = []

dataframe_doc2vec = pd.DataFrame(data = dict_dist)

plt.figure(figsize=(10, 7)) 
sns.heatmap(dataframe_doc2vec)

print("number of values above 0.5 in similarity score: ", counter)

dict_dist2 = {}
list_row2 = []
counter2 = 0

# bible and brown

for i in range(30):
  for j in range(500,530):
    vec1 = doc2vec_model.infer_vector(tokenized_data[i])
    vec2 = doc2vec_model.infer_vector(tokenized_data[j])
    spatial2 = spatial.distance.cosine(vec1, vec2)
    if spatial2 > 0.7:
      counter2 = counter2 + 1
    list_row2.append(spatial2)
  dict_dist2[i] = list_row2
  list_row2 = []

dataframe_doc2vec2 = pd.DataFrame(data = dict_dist2)

plt.figure(figsize=(10, 7)) 
sns.heatmap(dataframe_doc2vec2)

print("number of values above 0.5 in similarity score: ", counter2)